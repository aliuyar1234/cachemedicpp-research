\section{Introduction}

Efficient autoregressive decoding in transformers relies on the \emph{key--value (KV) cache}: at each layer, the model stores past keys and values and reuses them to avoid recomputing attention on previous tokens. This cache is not only a performance optimization; it is \textbf{persistent internal state} that is read repeatedly across time. Because cached keys/values are repeatedly reused, corruptions of this state can propagate through attention and alter subsequent predictions. Recent work has explored fault models and attacks that act directly on the KV cache, including transformer memory corruption, bit-flip-style cache faults, and history swapping \cite{hossain2025mti,nahian2025cachetrap,ganesh2025historyswap}.

We study \textbf{KV-cache integrity} under reproducible cache perturbations and ask a focused question:
\emph{Can we stabilize a frozen model's outputs by learning a small operator that repairs corrupted cache states before attention consumes them?}
We explicitly do \emph{not} address KV compression or eviction; our goal is robustness and integrity of the existing cache.

\paragraph{Approach.}
CacheMedic++ treats transformer inference with a KV cache as a dynamical system over persistent state $S_t$ (the collection of cached $(K,V)$ tensors). We introduce a lightweight \emph{stability operator} $R_\phi$ inserted \emph{inside attention}:
after past/current cache concatenation, we (optionally) corrupt the cache and then apply $R_\phi$ before attention logits are computed.
The base model parameters are frozen; we train only $\phi$ via self-distillation from clean teacher logits under a deterministic corruption family.
To go beyond generic denoising, we add an explicit \textbf{contraction regularizer} that encourages repaired cache states to contract toward the clean-cache manifold.

\paragraph{What we can claim (based on bundled evidence).}
On gpt2-medium under a leave-one-type-out (LOTO) protocol that holds out \texttt{contiguous\_overwrite} during training and uses it for evaluation, CacheMedic++ improves clean score from 0.2373 to 0.2466 and robustness AUC from 0.0390 (best heuristic) to 0.0401 (Table~\ref{tab:main}).
In a paired ablation that toggles only the contraction weight, contraction improves robustness AUC from 0.0388 to 0.0401 and reduces logit sensitivity relative to the no-contraction variant (Table~\ref{tab:ablations}, Fig.~\ref{fig:sens}).
A second-model run on gpt2-large shows qualitative transfer in the same direction (Table~\ref{tab:second_model}).
We emphasize paired ablations and OOD robustness/clean tradeoffs as our strongest evidence; \textbf{absolute stability versus the unmodified baseline is not the central win} in this setting.

\paragraph{Contributions.}
\begin{itemize}
  \item \textbf{Contractive KV stabilization.} We introduce CacheMedic++, a learned KV-cache stability operator $R_\phi$ inserted inside attention; base model weights remain frozen and only $R_\phi$ is trained.
  \item \textbf{Distillation + contraction training.} We train $R_\phi$ via self-distillation under a deterministic corruption family and add an explicit contraction regularizer toward clean-cache reference states.
  \item \textbf{Stability as a first-class metric.} We provide reproducible protocols for logit sensitivity curves and layer/head amplification maps, and we report paired contraction ablations and second-model transfer.
\end{itemize}

\begin{figure}[t]
  \centering
  \FigOrBox[width=0.95\linewidth]{figures/figA_score_vs_eps.png}
  \caption{\textbf{Robustness curves} on gpt2-medium under OOD held-out \texttt{contiguous\_overwrite} (LOTO). The robustness score is the mean of task scores (SST-2 accuracy, inverse Wikitext-2 perplexity, and needle accuracy; higher is better). Table~\ref{tab:main} summarizes clean score ($\varepsilon=0$) and robustness AUC (trapezoidal area under the curve over $\varepsilon \in \{0,0.08,0.16\}$).}
  \label{fig:robustness}
\end{figure}
