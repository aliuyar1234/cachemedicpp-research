\section{Experiments}

We report results on two frozen GPT-2 models (gpt2-medium and gpt2-large) using the harness protocols bundled with this paper.
All reported numbers are computed from the accompanying evidence JSON files (see Appendix~\ref{app:evidence_map}).

\subsection{Setup}
\paragraph{Models.}
We evaluate gpt2-medium as the primary model and gpt2-large as a second-model confirmation.
Inference uses batch size 1, eager attention, and the standard KV cache.

\paragraph{Tasks.}
We evaluate on:
(i) Wikitext-2 perplexity (400 examples),
(ii) SST-2 prompted classification (250 examples),
and (iii) a deterministic needle-style long-context probe (80 examples).
Exact prompting and subset sizes are specified in Appendix~\ref{app:eval_details}.

\paragraph{OOD protocol and corruption.}
We train CacheMedic++ on a fixed mixture of five corruption types and hold out \texttt{contiguous\_overwrite}.
Evaluation is performed on the held-out \texttt{contiguous\_overwrite} type at $\varepsilon\in\{0,0.08,0.16\}$ (LOTO; Sec.~\ref{sec:threat}).
For all runs, corruption applies only to the \texttt{old\_only} segment of the cache (all but the most recent 32 tokens), exposing long-range cache dependence.

\paragraph{Baselines and ablations.}
We compare against:
\textbf{no defense} and four inference-only heuristics (reset/clear, smoothing, masking/dropout, clipping/renorm).
We report \textbf{best heuristic} chosen by robustness AUC on the evaluation grid.
To test the stability framing, we include a \textbf{paired contraction ablation}: the same repair operator and training setup with $\lambda_{\mathrm{contr}}$ set to 0.

\subsection{Main results (gpt2-medium)}
Table~\ref{tab:main} summarizes clean score and robustness AUC.
CacheMedic++ improves both clean score and robustness AUC compared to no defense and the best heuristic baseline.
Figure~\ref{fig:robustness} shows the full robustness curve.

\begin{table}[t]
  \centering
  \caption{gpt2-medium results under OOD held-out \texttt{contiguous\_overwrite} (LOTO). Clean score is at $\varepsilon=0$; robustness AUC is trapezoidal area over $\varepsilon\in\{0,0.08,0.16\}$.}
  \label{tab:main}
  \vspace{2pt}
  \input{tables/table_main_results.tex}
\end{table}

\begin{table}[t]
  \centering
  \small
  \caption{Compact claim-to-evidence map in main text (key quantitative claims and their exact evidence files/keys).}
  \label{tab:claim_evidence_compact}
  \vspace{2pt}
  \input{tables/table_claim_evidence_compact.tex}
\end{table}

\subsection{Paired contraction ablation}
Table~\ref{tab:ablations} isolates the contraction regularizer by toggling only $\lambda_{\mathrm{contr}}$.
Contraction improves robustness AUC and reduces logit sensitivity relative to the no-contraction variant at both $\delta=1$ and $\delta=2$.
We stress that this paired comparison is the cleanest evidence that the stability term contributes beyond the distillation objective.

\begin{table}[t]
  \centering
  \caption{Paired contraction ablation on gpt2-medium (same operator and training setup; only $\lambda_{\mathrm{contr}}$ differs). ``Stability ratio'' is $\mathrm{Sens}_{\mathrm{contr}}(\delta)/\mathrm{Sens}_{\mathrm{no\ contr}}(\delta)$; values below 1 indicate improved stability.}
  \label{tab:ablations}
  \vspace{2pt}
  \input{tables/table_ablation_summary.tex}
\end{table}

\subsection{Three-seed replication (contr vs no\_contr)}
To test run-to-run stability, we repeated the same paired configuration with two additional seeds (2026 and 2027) and combined them with the canonical seed (1234).
Table~\ref{tab:seedrep} reports mean $\pm$ std over the 3-seed paired set.
The average robustness AUC gain is small ($+0.0001 \pm 0.0012$), and the mean sensitivity ratios are above 1 ($1.0584 \pm 0.1628$ at $\delta=1$, $1.0892 \pm 0.2004$ at $\delta=2$), indicating substantial seed variance and no consistent stability gain across all seeds.
We therefore treat the paired single-seed ablation as suggestive evidence and the 3-seed result as an explicit limitation that motivates stronger statistical reporting.

\begin{table}[t]
  \centering
  \small
  \caption{Three-seed replication for the paired contraction ablation (same tuned config, seeds 1234/2026/2027). Reported as mean $\pm$ std.}
  \label{tab:seedrep}
  \vspace{2pt}
  \input{tables/table_seed_replication_3seed.tex}
\end{table}

Table~\ref{tab:seedrep_ci} adds bootstrap uncertainty over paired seeds ($B=20{,}000$).
All intervals include no-effect thresholds (AUC gain $=0$, ratio $=1$), reinforcing that this effect is currently not statistically stable in our small-seed regime.

\begin{table}[t]
  \centering
  \caption{Bootstrap uncertainty for the 3-seed paired replication (percentile CI over seed pairs, $B=20{,}000$).}
  \label{tab:seedrep_ci}
  \vspace{2pt}
  \input{tables/table_seed_bootstrap_ci.tex}
\end{table}

\subsection{Mini hyper-sensitivity: $\lambda_{\mathrm{contr}}\in\{0,3,5\}$}
Table~\ref{tab:lambda_sensitivity} provides a compact one-seed hyper-sensitivity check under the same tuned setup.
The strongest setting is $\lambda_{\mathrm{contr}}=3.0$.
Pushing contraction to $\lambda_{\mathrm{contr}}=5.0$ degrades clean score and robustness AUC and increases sensitivity beyond both $\lambda_{\mathrm{contr}}=3.0$ and $\lambda_{\mathrm{contr}}=0.0$.
This is an explicit negative result and supports a ``sweet spot'' interpretation rather than monotonic gains from larger contraction weight.

\begin{table}[t]
  \centering
  \caption{Mini hyper-sensitivity on the tuned V-only setup (single-seed comparison).}
  \label{tab:lambda_sensitivity}
  \vspace{2pt}
  \input{tables/table_lambda_sensitivity.tex}
\end{table}

\subsection{Second model: gpt2-large}
Table~\ref{tab:second_model} reports the same evaluation protocol on gpt2-large.
CacheMedic++ shows a smaller but consistent improvement in both clean score and robustness AUC.

\begin{table}[t]
  \centering
  \caption{Second-model confirmation on gpt2-large under the same OOD held-out \texttt{contiguous\_overwrite} protocol.}
  \label{tab:second_model}
  \vspace{2pt}
  \input{tables/table_second_model_results.tex}
\end{table}

\subsection{Stability behavior and the ``absolute stability'' limitation}
Figure~\ref{fig:sens} reports logit sensitivity curves and includes the no-contraction ablation.
Contraction reduces sensitivity relative to the no-contraction variant (Table~\ref{tab:ablations}), but \textbf{absolute sensitivity relative to the unmodified baseline remains higher} in this setting.
We therefore frame CacheMedic++ primarily as a \emph{relative stabilization} method whose strongest evidence comes from paired ablations, robustness/clean tradeoffs, and second-model transfer rather than absolute stability improvements over the unmodified model.

Figure~\ref{fig:amp} further visualizes that amplification differences are concentrated in the protected layers (10--11), consistent with the operator being active only in that layer subset.
