\section{Experiments}

We report results on two frozen GPT-2 models (gpt2-medium and gpt2-large) using the harness protocols bundled with this paper.
All reported numbers are computed from the accompanying evidence JSON files (see Appendix~\ref{app:evidence_map}).

\subsection{Setup}
\paragraph{Models.}
We evaluate gpt2-medium as the primary model and gpt2-large as a second-model confirmation.
Inference uses batch size 1, eager attention, and the standard KV cache.

\paragraph{Tasks.}
We evaluate on:
(i) Wikitext-2 perplexity (400 examples),
(ii) SST-2 prompted classification (250 examples),
and (iii) a deterministic needle-style long-context probe (80 examples).
Exact prompting and subset sizes are specified in Appendix~\ref{app:eval_details}.

\paragraph{OOD protocol and corruption.}
We train CacheMedic++ on a fixed mixture of five corruption types and hold out \texttt{contiguous\_overwrite}.
Evaluation is performed on the held-out \texttt{contiguous\_overwrite} type at $\varepsilon\in\{0,0.08,0.16\}$ (LOTO; Sec.~\ref{sec:threat}).
For all runs, corruption applies only to the \texttt{old\_only} segment of the cache (all but the most recent 32 tokens), exposing long-range cache dependence.

\paragraph{Baselines and ablations.}
We compare against:
\textbf{no defense} and four inference-only heuristics (reset/clear, smoothing, masking/dropout, clipping/renorm).
We report \textbf{best heuristic} chosen by robustness AUC on the evaluation grid.
To test the stability framing, we include a \textbf{paired contraction ablation}: the same repair operator and training setup with $\lambda_{\mathrm{contr}}$ set to 0.

\subsection{Main results (gpt2-medium)}
Table~\ref{tab:main} summarizes clean score and robustness AUC.
CacheMedic++ improves both clean score and robustness AUC compared to no defense and the best heuristic baseline.
Figure~\ref{fig:robustness} shows the full robustness curve.

\begin{table}[t]
  \centering
  \caption{gpt2-medium results under OOD held-out \texttt{contiguous\_overwrite} (LOTO). Clean score is at $\varepsilon=0$; robustness AUC is trapezoidal area over $\varepsilon\in\{0,0.08,0.16\}$.}
  \label{tab:main}
  \vspace{2pt}
  \input{tables/table_main_results.tex}
\end{table}

\subsection{Paired contraction ablation}
Table~\ref{tab:ablations} isolates the contraction regularizer by toggling only $\lambda_{\mathrm{contr}}$.
Contraction improves robustness AUC and reduces logit sensitivity relative to the no-contraction variant at both $\delta=1$ and $\delta=2$.
We stress that this paired comparison is the cleanest evidence that the stability term contributes beyond the distillation objective.

\begin{table}[t]
  \centering
  \caption{Paired contraction ablation on gpt2-medium (same operator and training setup; only $\lambda_{\mathrm{contr}}$ differs). ``Stability ratio'' is $\mathrm{Sens}_{\mathrm{contr}}(\delta)/\mathrm{Sens}_{\mathrm{no\ contr}}(\delta)$; values below 1 indicate improved stability.}
  \label{tab:ablations}
  \vspace{2pt}
  \input{tables/table_ablation_summary.tex}
\end{table}

\subsection{Second model: gpt2-large}
Table~\ref{tab:second_model} reports the same evaluation protocol on gpt2-large.
CacheMedic++ shows a smaller but consistent improvement in both clean score and robustness AUC.

\begin{table}[t]
  \centering
  \caption{Second-model confirmation on gpt2-large under the same OOD held-out \texttt{contiguous\_overwrite} protocol.}
  \label{tab:second_model}
  \vspace{2pt}
  \input{tables/table_second_model_results.tex}
\end{table}

\subsection{Stability behavior and the ``absolute stability'' limitation}
Figure~\ref{fig:sens} reports logit sensitivity curves and includes the no-contraction ablation.
Contraction reduces sensitivity relative to the no-contraction variant (Table~\ref{tab:ablations}), but \textbf{absolute sensitivity relative to the unmodified baseline remains higher} in this setting.
We therefore frame CacheMedic++ primarily as a \emph{relative stabilization} method whose strongest evidence comes from paired ablations, robustness/clean tradeoffs, and second-model transfer rather than absolute stability improvements over the unmodified model.

Figure~\ref{fig:amp} further visualizes that amplification differences are concentrated in the protected layers (10--11), consistent with the operator being active only in that layer subset.
