\section{Experiments}

We evaluate CacheMedic++ on Wikitext-2 perplexity, SST-2 prompted classification, and a deterministic needle-style long-context check. We report robustness curves versus corruption severity and stability metrics.

\paragraph{Setup.}
Our primary model is gpt2-medium; second-model confirmation uses gpt2-large. The primary evaluation uses 400 Wikitext-2 examples, 250 SST-2 examples, and 80 long-context needle examples, with robustness scored as the mean of task scores (SST-2 accuracy, inverse perplexity for Wikitext-2, and needle accuracy).

\paragraph{Primary-model outcome (gpt2-medium).}
On the canonical tuned V-only run, CacheMedic++ improves both clean score and robustness AUC versus no defense and the best heuristic (Table~\ref{tab:main}): clean score rises from 0.2373 (no defense) and 0.2373 (best heuristic) to 0.2466, while robustness AUC rises from 0.0384 and 0.0390 to 0.0401.

\paragraph{Contraction ablation (paired no\_contr).}
Using the exact tuned pair, contraction improves robustness relative to the no-contraction ablation: robustness AUC increases from 0.0388 (no\_contr) to 0.0401 (contr), a +0.0013 absolute gain. Stability also improves relative to no\_contr with paired ratios 0.9291 at $\delta=1.0$ and 0.9266 at $\delta=2.0$. Clean WT2 regression remains small (+0.28\% PPL relative to no defense), staying inside the project clean-regression bound.

\paragraph{Interpretation.}
The strongest claim supported by the paired evidence is that contraction improves the repaired model relative to the identical no-contraction setup. Absolute sensitivity versus the unmodified baseline remains a limitation and is reported transparently as part of the paper's scope.

\paragraph{Second-model confirmation (gpt2-large).}
The same operator family and training recipe transfer qualitatively to gpt2-large (Table~\ref{tab:second_model}): clean score improves from 0.2974 to 0.3001 and robustness AUC from 0.0472 to 0.0475.

\begin{table}[t]
  \centering
  \InputOrBox{tables/table_main_results.tex}
  \caption{Main robustness and clean-score results on gpt2-medium.}
  \label{tab:main}
\end{table}

\begin{table}[t]
  \centering
  \InputOrBox{tables/table_ablation_summary.tex}
  \caption{Paired ablation summary (no\_contr vs contr) for the tuned V-only setup.}
  \label{tab:ablations}
\end{table}

\begin{table}[t]
  \centering
  \InputOrBox{tables/table_second_model_results.tex}
  \caption{Second-model confirmation (gpt2-large) from the tuned V-only setup.}
  \label{tab:second_model}
\end{table}
